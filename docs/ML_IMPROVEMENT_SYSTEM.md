# –°–∏—Å—Ç–µ–º–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏

## üéØ –¶–µ–ª—å
–ü–æ—Å—Ç–æ—è–Ω–Ω–æ —É–ª—É—á—à–∞—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.

## üß† –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã ML —Å–∏—Å—Ç–µ–º—ã

### 1. **–°–±–æ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö**

```python
class TrainingDataCollector:
    """–°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    
    def __init__(self):
        self.training_data = []
        self.validation_threshold = 100  # –ú–∏–Ω–∏–º—É–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    
    async def collect_correction(self, 
                                original_extraction: dict,
                                user_correction: dict,
                                document_path: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∫ –æ–±—É—á–∞—é—â–∏–π –ø—Ä–∏–º–µ—Ä"""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º features –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        features = await self._extract_features(document_path)
        
        training_example = {
            'features': features,
            'original': original_extraction,
            'corrected': user_correction,
            'timestamp': datetime.now(),
            'document_type': self._classify_document(document_path)
        }
        
        self.training_data.append(training_example)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        if len(self.training_data) >= self.validation_threshold:
            await self._trigger_retraining()
```

### 2. **Feature Engineering**

```python
class DocumentFeatureExtractor:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è ML"""
    
    def extract_features(self, document_text: str) -> dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        
        features = {
            # –¢–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            'text_length': len(document_text),
            'line_count': document_text.count('\n'),
            'has_table': self._detect_table_structure(document_text),
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã
            'vat_pattern_found': bool(re.search(r'[A-Z]{2}\d{8,12}', document_text)),
            'date_patterns': len(re.findall(r'\d{2}[-/.]\d{2}[-/.]\d{4}', document_text)),
            'amount_patterns': len(re.findall(r'\d+[.,]\d{2}', document_text)),
            
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            'invoice_keywords': self._count_keywords(document_text, INVOICE_KEYWORDS),
            'company_indicators': self._count_keywords(document_text, COMPANY_KEYWORDS),
            
            # –°—Ç—Ä—É–∫—Ç—É—Ä–∞
            'header_confidence': self._analyze_header_structure(document_text),
            'footer_confidence': self._analyze_footer_structure(document_text),
            
            # –Ø–∑—ã–∫
            'detected_language': self._detect_language(document_text),
            'language_confidence': self._language_confidence(document_text)
        }
        
        return features
```

### 3. **–ú–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á**

```python
class SpecializedModels:
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.models = {
            'company_matcher': CompanyMatchingModel(),
            'amount_extractor': AmountExtractionModel(),
            'date_parser': DateParsingModel(),
            'vat_validator': VATValidationModel(),
            'document_classifier': DocumentTypeClassifier()
        }
    
    async def train_all(self, training_data: list):
        """–û–±—É—á–∞–µ—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        for model_name, model in self.models.items():
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏
            relevant_data = self._filter_relevant_data(
                training_data, 
                model_name
            )
            
            if len(relevant_data) > model.min_training_size:
                await model.train(relevant_data)
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
                metrics = await model.validate()
                
                # –î–µ–ø–ª–æ–π —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É–ª—É—á—à–µ–Ω–∏–µ
                if metrics['accuracy'] > model.current_accuracy:
                    await model.deploy()
```

### 4. **Active Learning**

```python
class ActiveLearningManager:
    """–£–ø—Ä–∞–≤–ª—è–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º - –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ç–∫—É —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤"""
    
    def __init__(self):
        self.uncertainty_threshold = 0.3
        self.ambiguous_cases = []
    
    async def process_with_uncertainty(self, document_path: str) -> dict:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"""
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        predictions = await self._get_all_predictions(document_path)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        uncertainty = self._calculate_uncertainty(predictions)
        
        if uncertainty > self.uncertainty_threshold:
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å –¥–ª—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
            await self._queue_for_review(document_path, predictions, uncertainty)
            
            # –£–≤–µ–¥–æ–º–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            await self._notify_review_needed(
                document_path,
                reason="Low confidence extraction"
            )
        
        return predictions['best_guess']
```

### 5. **A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π**

```python
class ModelABTester:
    """A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ"""
    
    def __init__(self):
        self.experiments = {}
        self.traffic_split = 0.1  # 10% –Ω–∞ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
    
    async def process_with_experiment(self, document_path: str) -> dict:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å A/B —Ç–µ—Å—Ç–æ–º"""
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        use_experimental = random.random() < self.traffic_split
        
        if use_experimental and self.experiments:
            model = self.experiments['current']
            result = await model.process(document_path)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            await self._log_experiment_result(
                document_path,
                model_version='experimental',
                result=result
            )
        else:
            model = self.production_model
            result = await model.process(document_path)
            
            await self._log_experiment_result(
                document_path,
                model_version='production',
                result=result
            )
        
        return result
```

## üìä –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞

### –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏:
1. **Precision/Recall** –ø–æ —Ç–∏–ø–∞–º –ø–æ–ª–µ–π
2. **F1 Score** –¥–ª—è –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏
3. **Confusion Matrix** –¥–ª—è —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
4. **User Correction Rate** - —á–∞—Å—Ç–æ—Ç–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
5. **Processing Confidence** - —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏

### Dashboard –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:
```python
class MLMetricsDashboard:
    """–î–∞—à–±–æ—Ä–¥ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è ML –º–µ—Ç—Ä–∏–∫"""
    
    def generate_report(self) -> dict:
        return {
            'model_performance': {
                'company_matching': {
                    'accuracy': 0.95,
                    'precision': 0.93,
                    'recall': 0.97,
                    'f1_score': 0.95
                },
                'amount_extraction': {
                    'accuracy': 0.98,
                    'mean_error': 0.02,
                    'outliers': 3
                }
            },
            'training_stats': {
                'total_examples': 5420,
                'last_training': '2024-01-15',
                'next_scheduled': '2024-02-01'
            },
            'user_feedback': {
                'correction_rate': 0.05,
                'satisfaction_score': 4.7
            }
        }
```

## üöÄ –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π

### CI/CD pipeline –¥–ª—è ML:
1. **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ** –ø—Ä–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö
2. **–í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ**
3. **Canary deployment** - –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–π rollout
4. **–û—Ç–∫–∞—Ç –ø—Ä–∏ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫**

```yaml
# .github/workflows/ml-pipeline.yml
name: ML Model Training Pipeline

on:
  schedule:
    - cron: '0 2 * * 0'  # –ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ
  workflow_dispatch:

jobs:
  train:
    runs-on: ubuntu-latest
    steps:
      - name: Prepare training data
        run: python scripts/prepare_training_data.py
      
      - name: Train models
        run: python scripts/train_models.py
      
      - name: Validate performance
        run: python scripts/validate_models.py
      
      - name: Deploy if improved
        if: success()
        run: python scripts/deploy_models.py
```
