# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

## üéØ –¶–µ–ª—å
–û–±–µ—Å–ø–µ—á–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É 1000+ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —á–∞—Å —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–æ–π.

## ‚ö° –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### 1. **–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞**

```python
import asyncio
from concurrent.futures import ProcessPoolExecutor
import multiprocessing

class ParallelDocumentProcessor:
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.max_workers = multiprocessing.cpu_count()
        self.executor = ProcessPoolExecutor(max_workers=self.max_workers)
        self.semaphore = asyncio.Semaphore(self.max_workers * 2)
    
    async def process_batch(self, documents: List[str]) -> List[dict]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–∞–∫–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        chunk_size = max(1, len(documents) // self.max_workers)
        chunks = [documents[i:i + chunk_size] 
                 for i in range(0, len(documents), chunk_size)]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = []
        for chunk in chunks:
            task = self._process_chunk(chunk)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        return [item for sublist in results for item in sublist]
    
    async def _process_chunk(self, chunk: List[str]) -> List[dict]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–∞–Ω–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        
        async with self.semaphore:
            loop = asyncio.get_event_loop()
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
            result = await loop.run_in_executor(
                self.executor,
                self._process_documents_sync,
                chunk
            )
            
            return result
```

### 2. **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω—è—Ö**

```python
import redis
from functools import lru_cache
import hashlib

class MultiLevelCache:
    """–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ"""
    
    def __init__(self):
        self.redis_client = redis.Redis(decode_responses=True)
        self.local_cache = {}
        self.cache_stats = {'hits': 0, 'misses': 0}
    
    @lru_cache(maxsize=1000)
    def _compute_document_hash(self, document_path: str) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö–µ—à –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∫—ç—à–∞"""
        
        with open(document_path, 'rb') as f:
            return hashlib.sha256(f.read()).hexdigest()
    
    async def get_or_compute(self, 
                           document_path: str,
                           compute_func) -> dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–∑ –∫—ç—à–∞ –∏–ª–∏ –≤—ã—á–∏—Å–ª—è–µ—Ç"""
        
        # –£—Ä–æ–≤–µ–Ω—å 1: –õ–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à –≤ –ø–∞–º—è—Ç–∏
        doc_hash = self._compute_document_hash(document_path)
        
        if doc_hash in self.local_cache:
            self.cache_stats['hits'] += 1
            return self.local_cache[doc_hash]
        
        # –£—Ä–æ–≤–µ–Ω—å 2: Redis –∫—ç—à
        redis_key = f"doc:processed:{doc_hash}"
        cached_result = self.redis_client.get(redis_key)
        
        if cached_result:
            self.cache_stats['hits'] += 1
            result = json.loads(cached_result)
            self.local_cache[doc_hash] = result
            return result
        
        # –£—Ä–æ–≤–µ–Ω—å 3: –í—ã—á–∏—Å–ª—è–µ–º
        self.cache_stats['misses'] += 1
        result = await compute_func(document_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à–∏
        self.local_cache[doc_hash] = result
        self.redis_client.setex(
            redis_key,
            3600,  # TTL 1 —á–∞—Å
            json.dumps(result)
        )
        
        return result
```

### 3. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è OCR**

```python
class OptimizedOCR:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞"""
    
    def __init__(self):
        self.preprocessing_pipeline = [
            self._optimize_image_size,
            self._enhance_contrast,
            self._remove_noise,
            self._deskew
        ]
    
    async def extract_text_optimized(self, image_path: str) -> str:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
        
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        optimized_image = await self._preprocess_image(image_path)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±–ª–∞—Å—Ç–∏ —Å —Ç–µ–∫—Å—Ç–æ–º
        text_regions = await self._detect_text_regions(optimized_image)
        
        # OCR —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±–ª–∞—Å—Ç–µ–π —Å —Ç–µ–∫—Å—Ç–æ–º
        tasks = []
        for region in text_regions:
            task = self._ocr_region(optimized_image, region)
            tasks.append(task)
        
        texts = await asyncio.gather(*tasks)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        return self._combine_texts(texts, text_regions)
    
    async def _detect_text_regions(self, image) -> List[Region]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–±–ª–∞—Å—Ç–∏ —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º EAST text detector
        import cv2
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        net = cv2.dnn.readNet('models/frozen_east_text_detection.pb')
        
        # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–±–ª–∞—Å—Ç–∏
        # ... –∫–æ–¥ –¥–µ—Ç–µ–∫—Ü–∏–∏ ...
        
        return regions
```

### 4. **–ë–∞—Ç—á–∏–Ω–≥ API –∑–∞–ø—Ä–æ—Å–æ–≤**

```python
class APIBatchManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∞—Ç—á–∏–Ω–≥–æ–º API –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self):
        self.batch_size = 100
        self.batch_timeout = 0.5  # —Å–µ–∫—É–Ω–¥—ã
        self.pending_requests = []
        self.batch_processor_task = None
    
    async def add_request(self, request_data: dict) -> dict:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ –±–∞—Ç—á"""
        
        future = asyncio.Future()
        
        self.pending_requests.append({
            'data': request_data,
            'future': future
        })
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –µ—Å–ª–∏ –Ω–µ –∑–∞–ø—É—â–µ–Ω
        if not self.batch_processor_task:
            self.batch_processor_task = asyncio.create_task(
                self._process_batches()
            )
        
        # –ñ–¥–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        return await future
    
    async def _process_batches(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        
        while True:
            # –ñ–¥–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –±–∞—Ç—á–∞ –∏–ª–∏ —Ç–∞–π–º–∞—É—Ç
            await asyncio.sleep(self.batch_timeout)
            
            if not self.pending_requests:
                continue
            
            # –ë–µ—Ä–µ–º –±–∞—Ç—á
            batch = self.pending_requests[:self.batch_size]
            self.pending_requests = self.pending_requests[self.batch_size:]
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –±–∞—Ç—á –∑–∞–ø—Ä–æ—Å
            try:
                results = await self._send_batch_request(batch)
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                for item, result in zip(batch, results):
                    item['future'].set_result(result)
                    
            except Exception as e:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏
                for item in batch:
                    item['future'].set_exception(e)
```

### 5. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö**

```python
class DatabaseOptimizer:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å –ë–î"""
    
    def __init__(self):
        self.connection_pool = None
        self.prepared_statements = {}
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        
        self.connection_pool = await asyncpg.create_pool(
            min_size=10,
            max_size=20,
            max_queries=50000,
            max_inactive_connection_lifetime=300
        )
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —á–∞—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        await self._prepare_statements()
    
    async def _prepare_statements(self):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç SQL statements"""
        
        statements = {
            'insert_document': '''
                INSERT INTO documents (id, type, data, created_at)
                VALUES ($1, $2, $3, $4)
                ON CONFLICT (id) DO UPDATE
                SET data = $3, updated_at = NOW()
            ''',
            'get_by_hash': '''
                SELECT * FROM documents
                WHERE hash = $1 AND created_at > NOW() - INTERVAL '24 hours'
            '''
        }
        
        async with self.connection_pool.acquire() as conn:
            for name, sql in statements.items():
                self.prepared_statements[name] = await conn.prepare(sql)
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
class PerformanceMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
    
    def __init__(self):
        self.metrics = defaultdict(list)
        self.thresholds = {
            'document_processing_time': 5.0,  # —Å–µ–∫—É–Ω–¥—ã
            'api_response_time': 1.0,
            'memory_usage_mb': 4096
        }
    
    @contextmanager
    def measure(self, operation: str):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        try:
            yield
        finally:
            duration = time.time() - start_time
            memory_used = psutil.Process().memory_info().rss / 1024 / 1024 - start_memory
            
            self.metrics[f'{operation}_time'].append(duration)
            self.metrics[f'{operation}_memory'].append(memory_used)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥–∏
            if duration > self.thresholds.get(f'{operation}_time', float('inf')):
                logger.warning(f"Slow operation: {operation} took {duration:.2f}s")
```

## üöÄ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### –î–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
- **–°–∫–æ—Ä–æ—Å—Ç—å**: 50 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤/—á–∞—Å
- **CPU**: 80% –ø–æ—Å—Ç–æ—è–Ω–Ω–æ
- **RAM**: 8GB
- **Latency**: 30-60 —Å–µ–∫—É–Ω–¥

### –ü–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
- **–°–∫–æ—Ä–æ—Å—Ç—å**: 1200 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤/—á–∞—Å (24x)
- **CPU**: 40% —Å—Ä–µ–¥–Ω—è—è –∑–∞–≥—Ä—É–∑–∫–∞
- **RAM**: 4GB
- **Latency**: 2-5 —Å–µ–∫—É–Ω–¥

### –ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
1. **–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞** - 10x —É—Å–∫–æ—Ä–µ–Ω–∏–µ
2. **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ** - 50% –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ –∫—ç—à–∞
3. **–ë–∞—Ç—á–∏–Ω–≥ API** - 5x –º–µ–Ω—å—à–µ –∑–∞–ø—Ä–æ—Å–æ–≤
4. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è OCR** - 3x –±—ã—Å—Ç—Ä–µ–µ
5. **Connection pooling** - 2x –±—ã—Å—Ç—Ä–µ–µ –ë–î
